{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "04DxaHFaOPjI",
        "outputId": "bd28a60b-15f4-4e14-e69f-336e265c2641"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from kafka import KafkaConsumer\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.functions import from_json, col, when,mean,var_pop\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "from pyspark.sql.functions import to_timestamp, from_unixtime, unix_timestamp, row_number, desc\n",
        "from pymongo import MongoClient\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybYTVkzTTKLW"
      },
      "outputs": [],
      "source": [
        "conf = SparkConf().setAppName(\"Report 3 : Component Temperature Realtime Report\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ojTKHWcTKLb"
      },
      "outputs": [],
      "source": [
        "kafkaParams = {\n",
        "    \"bootstrap_servers\": \"ec2-65-0-72-75.ap-south-1.compute.amazonaws.com:9092\"}\n",
        "topic = \"IOTTemperatureStream01\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4lT5oFMTKLb"
      },
      "outputs": [],
      "source": [
        "consumer = KafkaConsumer(topic, **kafkaParams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlkY55r3hEbJ"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema=StructType(\n",
        "    [StructField(\"lane_number\", StringType(), True),\n",
        "        StructField(\"plant_name\", StringType(), True),\n",
        "        StructField(\"temperature\", IntegerType(), True),\n",
        "        StructField(\"timestamp\", TimestampType(), True),\n",
        "        StructField(\"component_type\", StringType(), True),\n",
        "        StructField(\"component_manufacturer\", StringType(), True),\n",
        "     ])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFRpc0a0TKLc"
      },
      "outputs": [],
      "source": [
        "def filterData(dataframe):\n",
        "    last_10_minutes = datetime.now() - timedelta(minutes=10)\n",
        "    last_30_minutes= datetime.now() - timedelta(minutes=30)\n",
        "    filtered_max_temp_data = dataframe.filter(dataframe.timestamp > last_30_minutes)\n",
        "    filtered_data = dataframe.filter(dataframe.timestamp > last_10_minutes)\n",
        "    filtered_data = filtered_data.filter(col(\"temperature\") > 50)\n",
        "    component_counts = filtered_data.groupBy(\"component_type\").count()\n",
        "    window2 = Window.partitionBy(\"lane_number\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "    window = Window.partitionBy(\"lane_number\").orderBy(desc(\"temperature\"))\n",
        "    filtered_max_temp_data = filtered_max_temp_data.withColumn(\"avg_value\", mean(\"temperature\").over(window2))\n",
        "    filtered_max_temp_data = filtered_max_temp_data.withColumn(\"variance\", var_pop(\"temperature\").over(window2))\n",
        "    filtered_max_temp_data.show()\n",
        "    max_temp_per_lane = filtered_max_temp_data.withColumn(\"row_number\", row_number().over(window))\n",
        "    max_temp_per_lane = max_temp_per_lane.filter(col(\"row_number\") == 1)\n",
        "    component_counts.show()\n",
        "    max_temp_per_lane.show()\n",
        "    gold_component_count = component_counts\n",
        "    gold_temp_per_lane = max_temp_per_lane\n",
        "    df_pandas = max_temp_per_lane.toPandas()\n",
        "    p = figure(title=\"Line Plot\", x_axis_label=\"lane Number\", y_axis_label=\"temperature\" )\n",
        "    p.line(df_pandas['lane_number'], df_pandas['temperature'],  legend_label=\"max temperature\")\n",
        "    p.line(df_pandas['lane_number'], df_pandas['avg_value'],  legend_label=\"avg temperature\")\n",
        "    output_notebook()\n",
        "    show(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKM5yvCuhPcF"
      },
      "outputs": [],
      "source": [
        "def process(rdd):\n",
        "    print(rdd)\n",
        "    global df\n",
        "    data = spark.read.json(rdd)\n",
        "    bronze_data = data\n",
        "    data = data.filter(data[\"component_info\"].isNotNull())\n",
        "    data = data.filter(data[\"timestamp\"].isNotNull())\n",
        "    if(data.count() > 0):\n",
        "        data = data.withColumn(\"timestamp\", when(col(\"timestamp\").cast(\"double\").isNotNull(\n",
        "        ), col(\"timestamp\").cast(\"double\").cast(\"timestamp\")).otherwise(col(\"timestamp\")))\n",
        "        data = data.withColumn(\"component_manufacturer\",\n",
        "                               data[\"component_info\"][\"component_manufacturer\"])\n",
        "        data = data.withColumn(\n",
        "            \"component_type\", data[\"component_info\"][\"component_type\"])\n",
        "        data = data.drop(\"component_info\")\n",
        "        df=df.union(data)\n",
        "    else:\n",
        "        print(\"No data\")\n",
        "    silver_data = df\n",
        "    filterData(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nknw77roTKLf"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    messages = consumer.poll(1000)\n",
        "    for tp, message in messages.items():\n",
        "        for record in message:\n",
        "            data = json.loads(record.value)\n",
        "            print(data)\n",
        "            if \"component_info\" in data and data[\"component_info\"] and \"component_type\" in data[\"component_info\"] and data[\"component_info\"][\"component_type\"] and data['temperature'] is not None:\n",
        "                rdd = sc.parallelize([record.value.decode('utf-8')])\n",
        "                process(rdd)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
